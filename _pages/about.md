---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I am a final-year Ph.D. student at the [Department of Computer Science, Emory University](https://www.cs.emory.edu/home/), where I am fortunate to be advised by Dr. [Liang Zhao](https://cs.emory.edu/~lzhao41/). Previously, I received my master's degree in Statistics from George Washington University in 2020. I received my bachelor's degree in Mathematics from the [School of Mathematical Science, Fudan University](https://math.fudan.edu.cn/) in Shanghai, China in 2018. I worked as a research intern at [Argonne National Laboratory](https://www.anl.gov/) and  [NEC Lab America](https://www.nec-labs.com/).

<span style="color: red;">News:</span> I am looking for Machine Learning Engineer / Applied Scientist roles starting anytime in 2025. Feel free to DM me!

Research Interests
======
I am interested in designing **efficient** and **generalizable** learning algorithms. Specifically, my current research topics include but are not limited to **1.** Developing various learning algorithms for knowledge/domain transfer, such as multi-task learning (MTL), domain adaptation (DA), and domain generalization (DG). **2.** Designing large-scale machine learning algorithms with enhanced efficiency, such as model compression & acceleration of LLMs and distributed training for deep neural networks. **3.** Online learning such as continual/lifelong learning with efficient memory replay and neuro-inspiration. 

Selected Projects
=====
## 1. Domain and Knowledge Transfer
Enhancing machine learning models' adaptability and effectiveness across various domains/tasks.
### a) Multi-Task Learning
- [**Sign-Regularized Multi-Task Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch89)   
  _SDM 2023_  
- [**Saliency-Regularized Deep Multi-Task Learning**](https://dl.acm.org/doi/pdf/10.1145/3534678.3539442)  
  _KDD 2022_  

### b) Domain Adaptation/Generalization
- [**Prompt-Based Domain Discrimination for Multi-source Time Series Domain Adaptation**](https://arxiv.org/abs/2312.12276)   
  _KDD 2024_
- [**Continuous Temporal Domain Generalization**](https://arxiv.org/abs/2405.16075)   
  _NeurIPS 2024_  
- [**Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks**](https://openreview.net/pdf?id=sWOsRj4nT1n)  
  _ICLR 2023 (<span style="color: red;">Oral</span>)_ 

## 2. Efficient Large-Scale Machine Learning
Exploring scalable solutions in machine learning.
### a) Model Compression & Acceleration of LLMs 
- [**Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models**](https://arxiv.org/abs/2401.00625)   
  _Under review of CSUR_
- [**SparseLLM: Towards Global Pruning for Pre-trained Language Models**](https://arxiv.org/pdf/2402.17946v3)   
  _NeurIPS 2024_ 

### b) Distributed Training for Deep Neural Networks
- [**Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction**](https://arxiv.org/pdf/2308.13466)  
  _Preprint_

## 3. Neuro-Inspired Continual Learning
Focusing on memory-replay and neuro-inspiration approaches for continual learning.
- [**Saliency-Guided Hidden Associative Replay for Continual Learning**](https://openreview.net/pdf?id=Fhx7nVoCQW)   
  _AMHN Workshop @NeurIPS 2023_  
- [**Saliency-Augmented Memory Completion for Continual Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch28)  
  _SDM 2023_  


Services and Awards
======
* PC member for KDD, ICML, ICLR, AISTATS, NeurIPS, AAAI, ICDM, etc.
* Primary writer for the [NSF NAIRR](https://nairrpilot.org/) 240189 grant ($15k) on parallel and distributed training of LLMs on graphs.
* KDD 22', ICLR 23', SDM 23', CIKM 23', NeurIPS 24' student travel award



