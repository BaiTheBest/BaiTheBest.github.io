---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I am a final-year Ph.D. student at the [Department of Computer Science, Emory University](https://www.cs.emory.edu/home/), where I am fortunate to be advised by Dr. [Liang Zhao](https://cs.emory.edu/~lzhao41/). Previously, I received my master's degree in Statistics from George Washington University in 2020. I received my bachelor's degree in Mathematics from the [School of Mathematical Science, Fudan University](https://math.fudan.edu.cn/) in Shanghai, China in 2018. I worked as a research intern at [Argonne National Laboratory](https://www.anl.gov/) and  [NEC Lab America](https://www.nec-labs.com/).

<span style="color: red;">**News**:</span> I am actively looking for Machine Learning Engineer / Applied Scientist / Research Scientist roles starting anytime in 2025. Feel free to DM me!

# Research Interests
I am passionate about designing **efficient** and **generalizable** learning algorithms with applications across diverse domains. My research focuses on three primary areas:

1. **Efficient Large-Scale Machine Learning:** Developing methods to optimize large-scale models, particularly focusing on model compression, inference optimization, and distributed training for deep learning systems, including LLMs.  
2. **Domain and Knowledge Transfer:** Enhancing machine learning models' adaptability across various domains/tasks, including multi-task learning, domain adaptation, and domain generalization, with a particular focus on data with temporal concept drift.  
3. **Neuro-Inspired Continual Learning:** Designing lifelong learning algorithms inspired by neuroscience, incorporating memory replay mechanisms for efficiency and robustness.

---

# Selected Projects

## 1. Efficient Large-Scale Machine Learning
Exploring scalable and efficient solutions for machine learning systems, with a focus on LLM inference optimization.

### a) Inference Optimization of LLMs
- [**SparseLLM: Towards Global Pruning for Pre-trained Language Models**](https://arxiv.org/pdf/2402.17946)  
  _NeurIPS 2024_
- [**FedSpaLLM: Federated Pruning of Large Language Models**](https://arxiv.org/pdf/2410.14852)  
  _Preprint_  
- [**Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models**](https://arxiv.org/abs/2401.00625)  
  _Under review of CSUR_

### b) Distributed Training for Graph Neural Networks
- [**Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction**](https://arxiv.org/pdf/2308.13466)  
  _SDM 2025_

---

## 2. Domain and Knowledge Transfer
Enhancing machine learning models' adaptability and effectiveness across various domains and tasks, with a focus on temporal domain generalization and multi-task learning.

### a) Domain Adaptation/Generalization
- [**Prompt-Based Domain Discrimination for Multi-source Time Series Domain Adaptation**](https://arxiv.org/abs/2312.12276)  
  _KDD 2024_  
- [**Continuous Temporal Domain Generalization**](https://arxiv.org/abs/2405.16075)  
  _NeurIPS 2024_  
- [**Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks**](https://openreview.net/pdf?id=sWOsRj4nT1n)  
  _ICLR 2023 (<span style="color: red;">Oral</span>)_

### b) Multi-Task Learning
- [**Sign-Regularized Multi-Task Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch89)  
  _SDM 2023_  
- [**Saliency-Regularized Deep Multi-Task Learning**](https://dl.acm.org/doi/pdf/10.1145/3534678.3539442)  
  _KDD 2022_

---

## 3. Neuro-Inspired Continual Learning
Developing lifelong learning approaches with efficient memory-replay mechanisms and neuro-inspiration.
- [**Saliency-Augmented Memory Completion for Continual Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch28)  
  _SDM 2023_



Services and Awards
======
* PC member for KDD, ICML, ICLR, AISTATS, NeurIPS, AAAI, ICDM, etc.
* Primary writer for the [NSF NAIRR](https://nairrpilot.org/) 240189 grant ($15k) on parallel and distributed training of LLMs on graphs.
* KDD 22', ICLR 23', SDM 23', CIKM 23', NeurIPS 24' student travel award



